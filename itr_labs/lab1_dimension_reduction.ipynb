{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4d25872-f8a6-488b-9c26-621950e2de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General spark\n",
    "from pyspark import SparkContext, SparkContext\n",
    "\n",
    "#Working with dataframes\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "\n",
    "#Dimension reduction and clustering\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.mllib.feature import StandardScaler as StandardScalerRDD\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "#Regular python libraries\n",
    "import pandas as pd\n",
    "import ast\n",
    "import warnings\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2d35ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working here\n"
     ]
    }
   ],
   "source": [
    "print('working here')\n",
    "path = r\"../data/interventions_sample.pkl\"\n",
    "\n",
    "with open(path, \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43a2b3c-2f45-449e-befc-318ef8998d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working here\n"
     ]
    }
   ],
   "source": [
    "print('working here')\n",
    "session_id = data['session_id'].to_list\n",
    "intervention_id = data['intervention_id'].to_list\n",
    "embeddings = [ast.literal_eval(emb) for emb in data['embeddings_str']]\n",
    "\n",
    "columns = ['session_id', 'intervention_id', 'embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('working here')\n",
    "interventions_df = spark.createDataFrame(data, schema=columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
