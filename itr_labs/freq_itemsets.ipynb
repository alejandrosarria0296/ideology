{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650b3d09-d993-46f0-955f-d3c0aa09df17",
   "metadata": {},
   "source": [
    "## Libraries and UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50671f3-274a-4292-8c97-e36b20b479ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/06 06:15:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"frequent_itemsets\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf20558a-342b-494b-b8b8-cc8effb5f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(tokens):\n",
    "    if not tokens:\n",
    "        return []\n",
    "    valid_word_pattern = re.compile(r\"^[a-zA-ZáéíóúñÁÉÍÓÚÑüÜ]+$\")\n",
    "    return [token for token in tokens if valid_word_pattern.match(token)]\n",
    "clean_tokens_udf = udf(clean_tokens, ArrayType(StringType()))\n",
    "\n",
    "def is_valid_string(token):\n",
    "    return isinstance(token, str) and bool(re.match(r\"^[a-zA-ZáéíóúñÁÉÍÓÚÑüÜ]+$\", token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c744242-8121-4512-b36b-fdd7de6ed1a4",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358db6d2-586d-4f1e-98bc-e65c202d9402",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"../data/interventions_sample.csv\"\n",
    "interventions_sample = spark.read.csv(path, header=True)\n",
    "int_df = interventions_sample.select(\"session_id\", \"intervention_id\", \n",
    "                                     \"intervention_text\", \"intervention_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7203c261-9312-4b0b-a636-f4957617ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"../data/sample_clusters.csv\" \n",
    "df = spark.read.csv(path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "771392a1-67db-43d6-bd0b-2fcc3785406d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------------+--------------------+-------+\n",
      "|intervention_id|    session_id|   intervention_text|  intervention_words|cluster|\n",
      "+---------------+--------------+--------------------+--------------------+-------+\n",
      "|         451824|gaceta_459 (7)|de acuerdo con el...|['acciones', 'cuá...|      0|\n",
      "|         451621|    gaceta_852|albán urbano luis...|['carlos', 'casti...|      2|\n",
      "|         451599|    gaceta_852|presidente el sig...|['artículos', 'av...|      4|\n",
      "|         451438| gaceta_68 (7)|perdone un segund...|['comisión', 'deb...|      1|\n",
      "|         451192|gaceta_456 (6)|antes de empezar ...|['asistir', 'comp...|      1|\n",
      "|         451071|   gaceta_1536|celular 318862010...|['aceta', 'celula...|      3|\n",
      "|         450768|gaceta_633 (1)|un segundo ya nos...|['activamos', 'al...|      0|\n",
      "|         450662|gaceta_390 (6)|doctora lo que pa...|['afectados', 'ci...|      0|\n",
      "|         450234| gaceta_49 (9)|continúa el señor...|['anglicana', 'ap...|      1|\n",
      "|         450076|gaceta_134 (7)|señor presidente,...|['después', 'día'...|      1|\n",
      "|         450007|    gaceta_634|:  es cierto este...|['agenda', 'cambi...|      0|\n",
      "|         449987|    gaceta_634|austed senador al...|['alirio', 'aquí'...|      1|\n",
      "|         449982|    gaceta_634|martes, 6 de juni...|['aquí', 'casa', ...|      0|\n",
      "|         449841| gaceta_21 (6)|gracias señor pre...|['breve', 'consid...|      0|\n",
      "|         449308|gaceta_116 (8)|gracias señor pre...|['asciende', 'cad...|      0|\n",
      "|         449293|gaceta_116 (8)|artículo 1°. obje...|['ambiental', 'am...|      0|\n",
      "|         448919|gaceta_449 (6)|que se tenga en c...|['actas', 'alguna...|      4|\n",
      "|         448844|    gaceta_513|gracias president...|['afectó', 'agend...|      0|\n",
      "|         448710|    gaceta_513|, por  jueves, 2 ...|['asocia', 'autor...|      3|\n",
      "|         448705|    gaceta_513|17.\\t al grado de...|['albeiro', 'alva...|      2|\n",
      "+---------------+--------------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_df = int_df.join(df.select(\"intervention_id\", \"cluster\"), on=\"intervention_id\", how=\"inner\")\n",
    "int_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ef5870-1553-4931-be03-4f3861e1542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- intervention_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- intervention_text: string (nullable = true)\n",
      " |-- intervention_words: string (nullable = true)\n",
      " |-- cluster: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "int_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9e787e-f252-4356-9aa0-5b97356d31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_stopwords = [\n",
    "    \"a\", \"al\", \"algo\", \"algunas\", \"algunos\", \"ante\", \"antes\", \"aquel\", \"aquella\",\n",
    "    \"aquellas\", \"aquellos\", \"aquí\", \"cada\", \"casi\", \"como\", \"con\", \"contra\",\n",
    "    \"cual\", \"cuales\", \"cuando\", \"cuanta\", \"cuantas\", \"cuanto\", \"cuantos\", \"de\",\n",
    "    \"del\", \"dentro\", \"donde\", \"dos\", \"el\", \"él\", \"ella\", \"ellas\", \"ellos\", \"en\",\n",
    "    \"entre\", \"esa\", \"esas\", \"ese\", \"eso\", \"esos\", \"esta\", \"estas\", \"este\",\n",
    "    \"estos\", \"lo\", \"los\", \"la\", \"las\", \"me\", \"mi\", \"mí\", \"mis\", \"nos\", \"nosotras\",\n",
    "    \"nosotros\", \"o\", \"otra\", \"otras\", \"otro\", \"otros\", \"para\", \"pero\", \"poco\",\n",
    "    \"por\", \"que\", \"qué\", \"se\", \"sí\", \"sin\", \"sobre\", \"su\", \"sus\", \"tu\", \"tú\",\n",
    "    \"tus\", \"un\", \"una\", \"unas\", \"uno\", \"unos\", \"vosotras\", \"vosotros\", \"vuestra\",\n",
    "    \"vuestras\", \"vuestro\", \"vuestros\", \"y\", \"ya\", \"senador\", \"presidente\", \"honorable\",\n",
    "    \"secretario\", \"honorable\", \"cámara\", \"comisión\", \"representante\", \"gracias\", \"comisión\",\n",
    "    \"palabra\", \"doctor\", \"tiene\", \"uso\", \"señor\", \"le\", \"usted\", \"doctora\", \"muchas\", \"es\", \"no\",\n",
    "    \"vamos\", \"muy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc45a81f-6843-48e6-b3e5-f01ba8137ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              tokens|     filtered_tokens|\n",
      "+--------------------+--------------------+\n",
      "|[de, acuerdo, con...|[acuerdo, observa...|\n",
      "|[albán, urbano, l...|[albán, urbano, l...|\n",
      "|[presidente, el, ...|[siguiente, bloqu...|\n",
      "|[perdone, un, seg...|[perdone, segundi...|\n",
      "|[antes, de, empez...|[empezar, orden, ...|\n",
      "|[celular, 3188620...|[celular, 3188620...|\n",
      "|[un, segundo, ya,...|[segundo, traslad...|\n",
      "|[doctora, lo, que...|[pasa, discusione...|\n",
      "|[continúa, el, se...|[continúa, marco,...|\n",
      "|[señor, president...|[presidente,, veo...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tokenize to create word \"baskets\"\n",
    "tokenizer = Tokenizer(inputCol=\"intervention_text\", outputCol=\"tokens\")\n",
    "\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"tokens\",\n",
    "                                     outputCol=\"filtered_tokens\",\n",
    "                                     stopWords=esp_stopwords)\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover])\n",
    "pipeline_model = pipeline.fit(int_df)\n",
    "\n",
    "result_df = pipeline_model.transform(int_df)\n",
    "result_df.select('tokens', 'filtered_tokens').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea75b63-67a8-4e3d-acba-d5cd01e7a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cluster_df_dict = {}\n",
    "cluster_labels = [row[0] for row in result_df.select(\"cluster\").distinct().collect()]\n",
    "\n",
    "for label in cluster_labels:\n",
    "    cluster_df = result_df.filter(result_df['cluster'] == label)\n",
    "    cleaned_rows_for_fpgrowth = []\n",
    "    for row in cluster_df.collect():\n",
    "        filtered_tokens = row['filtered_tokens']\n",
    "        cleaned_tokens = []\n",
    "        for token in filtered_tokens:\n",
    "            if is_valid_string(token) and token not in cleaned_tokens:\n",
    "                cleaned_tokens.append(token)\n",
    "        cleaned_rows_for_fpgrowth.append([cleaned_tokens])\n",
    "    cluster_df_dict[label] = spark.createDataFrame(cleaned_rows_for_fpgrowth, [\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ce211-86b6-4862-a45b-bfffa3df9edd",
   "metadata": {},
   "source": [
    "## FP Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166b120e-f812-4f4f-88d4-8a6264d7b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.1)\n",
    "for label, df in cluster_df_dict.items():\n",
    "    print(f\"Showing results for cluster {label}, with {df.count()} interventions\")\n",
    "    fpm = fp.fit(df)\n",
    "    fpm.freqItemsets.sort(\"freq\", ascending=False).show()\n",
    "    fpm.associationRules.sort(\"confidence\", ascending=False).show()\n",
    "    fpm.transform(cleaned_df_for_fpgrowth).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
